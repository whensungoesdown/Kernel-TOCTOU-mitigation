
\section{Limitations and Future Work}
\label{sec:limitation}

%\subsection{Leveraging SMAP in Vulnerability Finding}

\textbf{Leveraging SMAP in Vulnerability Finding  } Having a fast way to monitor memory accesses is very helpful when finding vulnerabilities such as kernel TOCTOU. Because this type of vulnerability has an obvious pattern. For example, the kernel reads a user address twice within one syscall. But as mentioned earlier, memory operations are too common to barely trigger any event. 

Existing methods including monitoring within a fully emulated virtual machine such as Bochs~\cite{lawton2003bochs}. It's easy to instrument both kernel and user code. Security researcher Mateusz Jurczyk did an amazing job using Bochs to find kernel TOCTOU and memory disclosure vulnerabilities~\cite{jurczyk2013identifying}~\cite{bochspwnreloaded}. But one drawback of x86 emulation is the performance, because it's a fully software emulated virtual machine, every instruction is interpreted and executed by software. With additional instrumentation, the system runs extremely slow. Hence it's difficult to make an comprehensive test since Windows apps may crash due to a slow GUI response.

An alternative way is to use data breakpoint. It's also called hardware breakpoint which traps the system when the CPU reads or writes the breakpoint~\cite{erickson2010effective}. The performance overhead is comparatively modest. But the number of data breakpoints is quit limited, for example, four data breakpoints per CPU in x86 architecture. So only several chosen locations can be monitored at a time.


%There is another proposal. First load a driver which sets up VM environment, puts OS run as a virtual machine, just like our project does. Then transfer into kernel mode, invalidate all user-mode data pages, for example, clear the “Present” flag for all top-level user-mode data page table entries. In such way that all kernel-mode access to 00000000-7fffffff yield a page fault exception. The hypervisor intercepts the page fault and set TF (Single Step) flag and restore the faulting page to let the OS successfully execute the faulting instruction. After that it intercepts DB exception and set the page back to invalidate.  But the drawback is also obvious, both the kernel and user mode will trigger the page faults exceptions, so in a multi-thread process, once one thread call into kernel mode, other user mode thread may be interrupted too often due to their data accessing.

We repurposed SMAP for vulnerability mitigation in this paper, now we want to further leverage this as a memory access monitor. Based on our test, we know that when a SMAP exception occurs, it's already too late to disable SAMP by setting the AC flag in EFLAGS register of the current thread. The same exception will keep coming back. Therefore, we need to add that page to kernel space just like what we do in the migitation. But we want to set the page back as soon as possible because we don't want to miss any further access on it. So we use the single-step trap to regain control. In the next instruction, we set the page back to it's original state, and record this access for later analysis. Now all the user memory accesses during a syscall can be monitored and the performance overhead is modest.

During the implementation, we found several bugs in win32k.sys module. For example, user data fetching without a "try \& catch", which may lead to a local DOS vulnerability if the attacker purposefully invalid the user pages during the syscall. We also found several double fetches with "read write" and "read read" patterns.

\textbf{IPI Mechanism  }Currently we reuse part of the functionality of Windows to flush TLB. Those internal functions are not exported and will change prototypes in further versions. For further work, we plan to implement the IPI mechanism.

\subsection{Discussion: Solving Write Conflict}

As mentioned in~\autoref{sec:design}, our current implementation for solving write conflict is by delaying the writing thread. At first, we tried to solve it by implementing a more fine-grain COW(Copy On Write) which based on thread instead of process. Normal COW works in the following way.

Two or more processes share read-only pages, when data is written to these pages, the kernel intercepts the write attempt and allocates a new physical page, initialized with the original data. This is based


The other idea is that by implementing COW(Copy On Write) more fine-grain that based on threads, we can provide protection and writing consent at the same time. Traditional COW that based on processes is kept as is, such as module sharing won't be affected. Considering performance overhead, only page protection collision as discussed above happens, new page tables will be established and they splits from original ones. There will have one new page table entry which provides a new physical page mapping for the writing thread. Then the writing thread can write to the page without affecting the user buffer parameter which previously submitted to a system call. When the previous system call ends, it can trace the pages that being protected and other page tables that generated due to the collisions, merges two physical pages (update the original physical page with new modification) and eliminates the new generated page tables for the writing thread.

We think that comparing to the writing thread, the system call initiating thread is the first one who accessed the collision page. When the syscall ends, new data from the writing thread should be updated to the original page. If the updated part coincide with the buffer of the previous syscall parameter, then the data inconsistency should be correctly handled by operating system by default, because the change of data happens after a complete syscall.

Well known facts about paging on x86 architecture, for each thread, the root of page tables is stored in CR3, by which CPU can translate virtual-physical page mappings automatically in hardware. CR3 are updated by OS thread scheduling code during thread switching. Since we don't want to modify the target operating system, we can use hypervisor to monitoring thread switching events just like what we did for process context switching. There is no direct VM exit event that represent thread switching, but indirectly OS will access certain data structure when scheduling. By monitoring pages that contains certain system data structures, thread switching can be monitored~\cite{pan2017digtool}.


